{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP57iT1xxpTOyRwcvOSc0A/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huguryildiz/MLbasics/blob/main/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Basics"
      ],
      "metadata": {
        "id": "oPO7fGAupVf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent"
      ],
      "metadata": {
        "id": "aBR6ZKR1pefA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcWlPwZvo5UU",
        "outputId": "1778253c-0468-48b1-9419-c139ad5f9f1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Cost = 1028.9745, w = 2.2691, b = 0.1652\n",
            "Iteration 100: Cost = 6.9422, w = 3.5975, b = 2.8642\n",
            "Iteration 200: Cost = 4.6208, w = 3.3741, b = 4.3501\n",
            "Iteration 300: Cost = 3.7627, w = 3.2382, b = 5.2534\n",
            "Iteration 400: Cost = 3.4456, w = 3.1557, b = 5.8026\n",
            "Iteration 500: Cost = 3.3284, w = 3.1054, b = 6.1365\n",
            "Iteration 600: Cost = 3.2850, w = 3.0749, b = 6.3396\n",
            "Iteration 700: Cost = 3.2690, w = 3.0564, b = 6.4630\n",
            "Iteration 800: Cost = 3.2631, w = 3.0451, b = 6.5380\n",
            "Iteration 900: Cost = 3.2609, w = 3.0382, b = 6.5836\n",
            "Cost = 3.2601, Final parameters: w = 3.0341, b = 6.6111\n"
          ]
        }
      ],
      "source": [
        "# GRADIENT DESCENT - SIMPLE EXAMPLE\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = 3 * x + 7 + np.random.randn(100) * 2  # y = 3x + 7 + noise\n",
        "\n",
        "# Initialize parameters\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Gradient Descent\n",
        "cost_history = []\n",
        "w_history = []\n",
        "b_history = []\n",
        "for i in range(iterations):\n",
        "    y_pred = w * x + b\n",
        "    error = y_pred - y\n",
        "    cost = np.mean(error ** 2)  # Mean Squared Error\n",
        "    cost_history.append(cost)\n",
        "    w_history.append(w)\n",
        "    b_history.append(b)\n",
        "\n",
        "    # Compute gradients\n",
        "    dw = (2 / len(x)) * np.sum(error * x)\n",
        "    db = (2 / len(x)) * np.sum(error)\n",
        "\n",
        "    # Update parameters\n",
        "    w -= learning_rate * dw\n",
        "    b -= learning_rate * db\n",
        "\n",
        "    # Print status every 100 iterations\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Iteration {i}: Cost = {cost:.4f}, w = {w:.4f}, b = {b:.4f}\")\n",
        "\n",
        "# Final parameter values\n",
        "print(f\"Cost = {cost:.4f}, Final parameters: w = {w:.4f}, b = {b:.4f}\")\n",
        "\n",
        "# Plot cost function in 3D\n",
        "w_values = np.linspace(w - 2, w + 2, 50)\n",
        "b_values = np.linspace(b - 2, b + 2, 50)\n",
        "J_values = np.zeros((50, 50))\n",
        "\n",
        "for i in range(50):\n",
        "    for j in range(50):\n",
        "        w_temp = w_values[i]\n",
        "        b_temp = b_values[j]\n",
        "        y_pred_temp = w_temp * x + b_temp\n",
        "        J_values[i, j] = np.mean((y_pred_temp - y) ** 2)\n",
        "\n",
        "W, B = np.meshgrid(w_values, b_values)\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(W, B, J_values, cmap='turbo', alpha=1)\n",
        "ax.set_xlabel('Weight (w)')\n",
        "ax.set_ylabel('Bias (b)')\n",
        "ax.set_zlabel('Cost J(w,b)')\n",
        "ax.set_title('Cost Function Surface')\n",
        "ax.view_init(elev=25, azim=135)  # Adjust these values for a better perspective\n",
        "# Plot gradient descent path\n",
        "ax.scatter(w_history, b_history, cost_history, color='red', marker='o', s=5, label='Gradient Descent Path')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Gradient Descent for Multivariable Regression"
      ],
      "metadata": {
        "id": "6fU5RB4A3N04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cost function for multivariable linear regression is defined as:\n",
        "\n",
        "\n",
        "$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)^2\n",
        "\\ $\n",
        "\n",
        "where:\n",
        "\n",
        "$\n",
        "h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n = \\bf{\\theta}^T X\n",
        "\\ $\n",
        "\n",
        "\n",
        "*   $ J(\\theta) $ is the cost function,\n",
        "*   $m $ is the number of training examples\n",
        "*   $n $ is the number of features\n",
        "*   $h_{\\theta}(x^{(i)})$ is the hypothesis function\n",
        "*   $y^{(i)}$ is the actual target value,\n",
        "*   $X$ is the feature matrix including the bias term ($\\theta_0$) [mxn]\n",
        "*   $\\theta$ is the parameter vector [1xn]\n",
        "*   $\\nabla J(\\theta)$ is the gradient vector [nx1]\n",
        "\n",
        "\n",
        "The gradient of $ J(\\theta) $ with respect to $ \\theta_j $ is:\n",
        "\n",
        "$\n",
        "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)}, j = 0 \\ldots n-1\n",
        " $\n",
        "\n",
        "Or in vectorized form:\n",
        "\n",
        "$\n",
        "\\nabla J(\\theta) = \\frac{1}{m} X^T (X\\theta - y)\n",
        " $\n",
        "\n",
        "Repeat until convergence: $\\{\n",
        "\\begin{equation}\n",
        "    \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) \\cdot x_j^{(i)}\n",
        "    \\quad \\text{for } j := 0 \\ldots n-1\n",
        "\\end{equation}\n",
        "\\} $\n",
        "\n",
        "Check:\n",
        "\n",
        "1. https://mrandri19.github.io/2019/04/01/deriving-gradient-descent-multivariate-linear-regression.html\n",
        "2. https://medium.com/@IwriteDSblog/gradient-descent-for-multivariable-regression-in-python-d430eb5d2cd8"
      ],
      "metadata": {
        "id": "NvO4sXaY4J-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_cost(X, y, theta):\n",
        "    \"\"\"\n",
        "    Compute the cost function for linear regression.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    predictions = X.dot(theta)  # Matrix multiplication\n",
        "    errors = predictions - y\n",
        "    cost = (1 / (2 * m)) * np.sum(errors ** 2)\n",
        "    return cost\n",
        "\n",
        "def gradient_descent(X, y, theta, alpha, num_iters, print_every=100):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize theta.\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    J_history = []\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        gradient = (1 / m) * X.T.dot(X.dot(theta) - y)  # Vectorized gradient computation\n",
        "        theta -= alpha * gradient\n",
        "        J_history.append(compute_cost(X, y, theta))\n",
        "\n",
        "        # Print results at specified intervals\n",
        "        if (i + 1) % print_every == 0:\n",
        "            print(f\"Iteration {i+1}: Cost = {J_history[-1]:.6f}, Theta = {theta} \\n\")\n",
        "\n",
        "    return theta, J_history\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate a random dataset with m=4 samples and n=10 features\n",
        "m, n = 10, 4\n",
        "X = np.random.rand(m, n)\n",
        "\n",
        "# Add bias term (theta_0) to X\n",
        "X = np.c_[np.random.rand(m, 1), X]\n",
        "\n",
        "# Generate random target values\n",
        "y = np.random.rand(m)\n",
        "\n",
        "# Initialize theta randomly\n",
        "theta = np.random.rand(n + 1)\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "num_iters = 5000  # Number of iterations\n",
        "\n",
        "print(f\" X = {X} \\n\")\n",
        "print(f\" y = {y} \\n\")\n",
        "print(f\" theta = {theta} \\n\")\n",
        "\n",
        "# Perform gradient descent\n",
        "optimal_theta, cost_history = gradient_descent(X, y, theta, alpha, num_iters)\n",
        "\n",
        "# Display final results\n",
        "print(\"\\nFinal Theta:\", optimal_theta)\n",
        "print(\"Final Cost:\", cost_history[-1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBFk9ju_2-5a",
        "outputId": "f77b687a-1df2-4a99-8410-f24d69766266"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " X = [[0.12203823 0.37454012 0.95071431 0.73199394 0.59865848]\n",
            " [0.49517691 0.15601864 0.15599452 0.05808361 0.86617615]\n",
            " [0.03438852 0.60111501 0.70807258 0.02058449 0.96990985]\n",
            " [0.9093204  0.83244264 0.21233911 0.18182497 0.18340451]\n",
            " [0.25877998 0.30424224 0.52475643 0.43194502 0.29122914]\n",
            " [0.66252228 0.61185289 0.13949386 0.29214465 0.36636184]\n",
            " [0.31171108 0.45606998 0.78517596 0.19967378 0.51423444]\n",
            " [0.52006802 0.59241457 0.04645041 0.60754485 0.17052412]\n",
            " [0.54671028 0.06505159 0.94888554 0.96563203 0.80839735]\n",
            " [0.18485446 0.30461377 0.09767211 0.68423303 0.44015249]] \n",
            "\n",
            " y = [0.96958463 0.77513282 0.93949894 0.89482735 0.59789998 0.92187424\n",
            " 0.0884925  0.19598286 0.04522729 0.32533033] \n",
            "\n",
            " theta = [0.38867729 0.27134903 0.82873751 0.35675333 0.28093451] \n",
            "\n",
            "Iteration 100: Cost = 0.053449, Theta = [ 0.33148146  0.51336996  0.29014402 -0.15460217  0.24508331] \n",
            "\n",
            "Iteration 200: Cost = 0.046753, Theta = [ 0.27096828  0.65826256  0.16396269 -0.23993096  0.37005219] \n",
            "\n",
            "Iteration 300: Cost = 0.044591, Theta = [ 0.20108162  0.73156839  0.08970094 -0.24846599  0.44325603] \n",
            "\n",
            "Iteration 400: Cost = 0.043552, Theta = [ 0.14394585  0.77870685  0.03796893 -0.23506207  0.487531  ] \n",
            "\n",
            "Iteration 500: Cost = 0.042986, Theta = [ 9.97810484e-02  8.12573136e-01  2.34318554e-04 -2.17806591e-01\n",
            "  5.15925407e-01] \n",
            "\n",
            "Iteration 600: Cost = 0.042667, Theta = [ 0.06601354  0.83798404 -0.02761613 -0.20220583  0.53508557] \n",
            "\n",
            "Iteration 700: Cost = 0.042485, Theta = [ 0.04025845  0.85735001 -0.04826114 -0.189502    0.54848743] \n",
            "\n",
            "Iteration 800: Cost = 0.042382, Theta = [ 0.02063522  0.87217801 -0.06361072 -0.17957962  0.55808384] \n",
            "\n",
            "Iteration 900: Cost = 0.042323, Theta = [ 0.00569745  0.88353644 -0.07505391 -0.17197017  0.5650598 ] \n",
            "\n",
            "Iteration 1000: Cost = 0.042289, Theta = [-0.0056637   0.89222766 -0.08360589 -0.16618239  0.57018166] \n",
            "\n",
            "Iteration 1100: Cost = 0.042269, Theta = [-0.01429772  0.89886783 -0.09001111 -0.16179631  0.57396814] \n",
            "\n",
            "Iteration 1200: Cost = 0.042258, Theta = [-0.02085468  0.90393326 -0.09481748 -0.15847757  0.57678134] \n",
            "\n",
            "Iteration 1300: Cost = 0.042252, Theta = [-0.02583134  0.90779213 -0.09842982 -0.15596784  0.57887929] \n",
            "\n",
            "Iteration 1400: Cost = 0.042248, Theta = [-0.02960673  0.91072841 -0.10114835 -0.15407015  0.58044839] \n",
            "\n",
            "Iteration 1500: Cost = 0.042246, Theta = [-0.03246967  0.91296054 -0.10319646 -0.15263513  0.58162466] \n",
            "\n",
            "Iteration 1600: Cost = 0.042245, Theta = [-0.03463999  0.91465602 -0.10474088 -0.15154984  0.58250808] \n",
            "\n",
            "Iteration 1700: Cost = 0.042244, Theta = [-0.03628481  0.91594305 -0.10590632 -0.15072893  0.58317255] \n",
            "\n",
            "Iteration 1800: Cost = 0.042244, Theta = [-0.03753111  0.91691952 -0.10678632 -0.1501079   0.58367294] \n",
            "\n",
            "Iteration 1900: Cost = 0.042244, Theta = [-0.03847528  0.91766005 -0.1074511  -0.14963803  0.58405014] \n",
            "\n",
            "Iteration 2000: Cost = 0.042243, Theta = [-0.03919046  0.91822146 -0.1079535  -0.14928249  0.58433471] \n",
            "\n",
            "Iteration 2100: Cost = 0.042243, Theta = [-0.03973213  0.91864695 -0.10833331 -0.14901344  0.58454953] \n",
            "\n",
            "Iteration 2200: Cost = 0.042243, Theta = [-0.04014235  0.91896937 -0.10862051 -0.14880983  0.58471179] \n",
            "\n",
            "Iteration 2300: Cost = 0.042243, Theta = [-0.04045299  0.91921363 -0.10883774 -0.14865572  0.5848344 ] \n",
            "\n",
            "Iteration 2400: Cost = 0.042243, Theta = [-0.04068822  0.91939866 -0.10900206 -0.14853909  0.58492708] \n",
            "\n",
            "Iteration 2500: Cost = 0.042243, Theta = [-0.04086632  0.9195388  -0.10912638 -0.1484508   0.58499715] \n",
            "\n",
            "Iteration 2600: Cost = 0.042243, Theta = [-0.04100118  0.91964493 -0.10922045 -0.14838398  0.58505015] \n",
            "\n",
            "Iteration 2700: Cost = 0.042243, Theta = [-0.04110328  0.9197253  -0.10929164 -0.1483334   0.58509023] \n",
            "\n",
            "Iteration 2800: Cost = 0.042243, Theta = [-0.04118058  0.91978616 -0.10934551 -0.14829511  0.58512056] \n",
            "\n",
            "Iteration 2900: Cost = 0.042243, Theta = [-0.0412391   0.91983224 -0.10938628 -0.14826612  0.58514351] \n",
            "\n",
            "Iteration 3000: Cost = 0.042243, Theta = [-0.04128341  0.91986714 -0.10941714 -0.14824418  0.58516087] \n",
            "\n",
            "Iteration 3100: Cost = 0.042243, Theta = [-0.04131696  0.91989355 -0.1094405  -0.14822758  0.58517401] \n",
            "\n",
            "Iteration 3200: Cost = 0.042243, Theta = [-0.04134235  0.91991355 -0.10945818 -0.148215    0.58518396] \n",
            "\n",
            "Iteration 3300: Cost = 0.042243, Theta = [-0.04136158  0.9199287  -0.10947156 -0.14820549  0.58519149] \n",
            "\n",
            "Iteration 3400: Cost = 0.042243, Theta = [-0.04137613  0.91994016 -0.10948169 -0.14819828  0.58519718] \n",
            "\n",
            "Iteration 3500: Cost = 0.042243, Theta = [-0.04138715  0.91994884 -0.10948936 -0.14819283  0.5852015 ] \n",
            "\n",
            "Iteration 3600: Cost = 0.042243, Theta = [-0.04139549  0.91995541 -0.10949517 -0.1481887   0.58520476] \n",
            "\n",
            "Iteration 3700: Cost = 0.042243, Theta = [-0.0414018   0.91996038 -0.10949956 -0.14818557  0.58520723] \n",
            "\n",
            "Iteration 3800: Cost = 0.042243, Theta = [-0.04140658  0.91996415 -0.10950289 -0.14818321  0.5852091 ] \n",
            "\n",
            "Iteration 3900: Cost = 0.042243, Theta = [-0.0414102   0.919967   -0.10950541 -0.14818142  0.58521052] \n",
            "\n",
            "Iteration 4000: Cost = 0.042243, Theta = [-0.04141294  0.91996916 -0.10950731 -0.14818006  0.58521159] \n",
            "\n",
            "Iteration 4100: Cost = 0.042243, Theta = [-0.04141502  0.91997079 -0.10950875 -0.14817903  0.5852124 ] \n",
            "\n",
            "Iteration 4200: Cost = 0.042243, Theta = [-0.04141659  0.91997203 -0.10950985 -0.14817826  0.58521301] \n",
            "\n",
            "Iteration 4300: Cost = 0.042243, Theta = [-0.04141778  0.91997297 -0.10951067 -0.14817767  0.58521348] \n",
            "\n",
            "Iteration 4400: Cost = 0.042243, Theta = [-0.04141868  0.91997367 -0.1095113  -0.14817722  0.58521383] \n",
            "\n",
            "Iteration 4500: Cost = 0.042243, Theta = [-0.04141936  0.91997421 -0.10951177 -0.14817689  0.5852141 ] \n",
            "\n",
            "Iteration 4600: Cost = 0.042243, Theta = [-0.04141987  0.91997462 -0.10951213 -0.14817663  0.5852143 ] \n",
            "\n",
            "Iteration 4700: Cost = 0.042243, Theta = [-0.04142026  0.91997492 -0.1095124  -0.14817644  0.58521445] \n",
            "\n",
            "Iteration 4800: Cost = 0.042243, Theta = [-0.04142056  0.91997516 -0.10951261 -0.14817629  0.58521457] \n",
            "\n",
            "Iteration 4900: Cost = 0.042243, Theta = [-0.04142078  0.91997533 -0.10951277 -0.14817618  0.58521465] \n",
            "\n",
            "Iteration 5000: Cost = 0.042243, Theta = [-0.04142095  0.91997547 -0.10951288 -0.1481761   0.58521472] \n",
            "\n",
            "\n",
            "Final Theta: [-0.04142095  0.91997547 -0.10951288 -0.1481761   0.58521472]\n",
            "Final Cost: 0.04224320245759676\n"
          ]
        }
      ]
    }
  ]
}