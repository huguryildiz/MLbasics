# Lab 3 - Neural Network Training and Optimization

## 📌 Overview  
This repository contains **Lab 3 - Neural Networks**, where we implement and train **multi-layer feedforward neural networks** using **TensorFlow/Keras**. The notebook covers key aspects of **deep learning**, including **model architecture, activation functions, loss functions, optimization, and evaluation**.

---

## 📂 Files  
**Lab3-NeuralNetworks.ipynb** → Jupyter Notebook implementing:  
- **Data Preprocessing**: Normalization, feature scaling, and train-test split  
- **Neural Network Construction**: Defining layers, activation functions, and loss functions  
- **Model Training**: Compiling and optimizing the model using different optimizers  
- **Performance Evaluation**: Accuracy, loss curves, and confusion matrix analysis  
- **Hyperparameter Tuning**: Adjusting learning rate, batch size, and regularization  

---

## 🚀 Usage  
Run the notebook to build and train a **neural network classifier**. Experiment with **different activation functions, optimizers, and regularization techniques** to analyze their impact on model performance.

---

## 📊 Key Results  
- A **basic neural network model** achieves successful classification.  
- **ReLU activation** improves convergence over sigmoid/tanh.  
- **Regularization (L2, dropout)** helps prevent overfitting.  
- **Tuning hyperparameters** significantly impacts training performance.  

---

## 🤝 Contributing  
Feel free to **fork** this repository, make improvements, and submit a **pull request**! 🚀  

---

## 🔧 Setup & Requirements  
To run the notebook, install the required dependencies:  
```bash
pip install tensorflow numpy pandas matplotlib scikit-learn
```
Then, open the notebook:  
```bash
jupyter notebook Lab3-NeuralNetworks.ipynb
```
