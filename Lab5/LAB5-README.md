# Lab 5 - Decision Trees

## 📌 Overview  
This repository contains **Lab 5 - Decision Trees**, where we explore **decision tree classifiers, tree visualization, feature importance, and model evaluation**. The notebook covers **training decision trees, understanding splitting criteria (entropy, Gini), pruning techniques, and hyperparameter tuning** to optimize performance.

---

## 📂 Files  
**Lab5-DecisionTrees.ipynb** → Jupyter Notebook implementing:  
- **Decision Tree Classifier**: Training and evaluating decision trees  
- **Splitting Criteria**: Comparing **Gini impurity** vs. **Entropy**  
- **Tree Visualization**: Using **`plot_tree`** to interpret decision paths  
- **Feature Importance**: Analyzing key factors influencing predictions  
- **Hyperparameter Tuning**: Adjusting max depth, min samples split, and pruning strategies  
- **Random Forests**: Comparing single trees vs. ensemble methods  

---

## 🚀 Usage  
Run the notebook to **train and analyze decision trees**, evaluate feature importance, and optimize hyperparameters for improved classification performance.

---

## 📊 Key Results  
- **Entropy and Gini impurity provide different decision splits** based on dataset characteristics.  
- **Pruning techniques (pre-pruning & post-pruning) help prevent overfitting** in decision trees.  
- **Feature importance analysis highlights the most influential attributes in predictions.**  
- **Random forests often generalize better than individual decision trees.**  

---

## 🤝 Contributing  
Feel free to **fork** this repository, make improvements, and submit a **pull request**! 🚀  

---

## 🔧 Setup & Requirements  
To run the notebook, install the required dependencies:  
```bash
pip install numpy pandas scikit-learn matplotlib seaborn xgboost
