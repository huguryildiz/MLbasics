# Lab 5 - Decision Trees

## ğŸ“Œ Overview  
This repository contains **Lab 5 - Decision Trees**, where we explore **decision tree classifiers, tree visualization, feature importance, and model evaluation**. The notebook covers **training decision trees, understanding splitting criteria (entropy, Gini), pruning techniques, and hyperparameter tuning** to optimize performance.

---

## ğŸ“‚ Files  
**Lab5-DecisionTrees.ipynb** â†’ Jupyter Notebook implementing:  
- **Decision Tree Classifier**: Training and evaluating decision trees  
- **Splitting Criteria**: Comparing **Gini impurity** vs. **Entropy**  
- **Tree Visualization**: Using **`plot_tree`** to interpret decision paths  
- **Feature Importance**: Analyzing key factors influencing predictions  
- **Hyperparameter Tuning**: Adjusting max depth, min samples split, and pruning strategies  
- **Random Forests**: Comparing single trees vs. ensemble methods  

---

## ğŸš€ Usage  
Run the notebook to **train and analyze decision trees**, evaluate feature importance, and optimize hyperparameters for improved classification performance.

---

## ğŸ“Š Key Results  
- **Entropy and Gini impurity provide different decision splits** based on dataset characteristics.  
- **Pruning techniques (pre-pruning & post-pruning) help prevent overfitting** in decision trees.  
- **Feature importance analysis highlights the most influential attributes in predictions.**  
- **Random forests often generalize better than individual decision trees.**  

---

## ğŸ¤ Contributing  
Feel free to **fork** this repository, make improvements, and submit a **pull request**! ğŸš€  

---

## ğŸ”§ Setup & Requirements  
To run the notebook, install the required dependencies:  
```bash
pip install numpy pandas scikit-learn matplotlib seaborn xgboost
